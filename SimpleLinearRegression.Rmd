---
title: "Simple Linear Regression"
output: 
  html_notebook: 
    number_sections: FALSE
    
---
```{r}
# install.packages("dplyr")
# install.packages("magrittr")
library(dplyr)
library(magrittr) # for the pipe
```


### Use the file* **penguins.exam.csv** for this problem.  It contains body measurements for over 100 Gentoo penguins observed in the Palmer Archipelago of Antarctica from 2007-2009.  The images show what the measurements mean.  Here we will work with bill length and flipper length.  

![](penguinbill.png)

![](flipper.png)

#### (a) Fit a SLR model where bill length is the predictor and flipper_length is the response. Display the regression output.  Compute 98\% confidence intervals for the intercept and slope coefficients.  

```{r}
###
penguins.exam <- read.csv("~/Desktop/stat5310/mid term /mid term test/penguins.exam.csv")
head(penguins.exam)
dim(penguins.exam)

penguins.mod <- lm(flipper_length_mm~bill_length_mm, data = penguins.exam) ##fit the model
summary(penguins.mod) ##display the result

confint(penguins.mod, level = 0.98) ##CI's for intercept and slope

summary(penguins.mod)$sigma ##estimate of sigma
```

#### (b) What proportion of the variation in flipper length is explained by bill length? 

##### ----------As we can see from the display of regression output, the model can explain the 45.34% of fliiper length on bill length.-----------

#### (c) What is the estimate of $\sigma$, the standard deviation of the errors? 

##### ----------The estimate sigma is 4.75245.-----------

#### (d) Make a plot with the data points and the fitted regression line.  

```{r}
plot(flipper_length_mm~bill_length_mm, data = penguins.exam, col='deeppink'); abline(penguins.mod, col = 'darkblue', lwd = 2)
grid()
```



### Problem 
#### (a) Make a data.frame called `mydata` with 5 rows and 2 columns called `xvec` and `yvec`.  The values in `xvec` are given below.  You pick the values in `yvec`. Select $y$ values so that the slope coefficent is negative when you fit the simple linear regression model `yvec ~ xvec`.  Display the regression output for your fitted model.  
```{r}
xvec <- c(2.2, 4.0, 5.6, 8.1, 9.6)

#yvec <- ???
yvec <- c(8.2, 7.1, 6.3, 3.2, 2.1) 

#data frame
mydata <- data.frame(xvec, yvec)

#display the regression output
mydata.mod = lm(yvec~xvec, data = mydata)
summary(mydata.mod)

#make scatter plot
plot(yvec~xvec, data = mydata, col='deeppink')

```


#### (b) Is there a significant linear relationship between `yvec` and `xvec`?  Justify your answer.   

##### ----------The p-value of the slope is 0.001644 which is nearly 0 (less than 0.05). Also, the model can explain 97.55% of yxec on xvec, therefore we can conclude that there is a statistically significant linear relationship between yvec and xvec -----------

### Problem
#### (a) (5 pts) Using your `yvec ~ xvec` model from the previous question, find the leverages of the five data points.  Identify any points which are high leverage. 

##### ----------After running diagnotic  plot for the model, we found no high leverage point even though we can see the observation numbered as #3 is being flagged in the diagnostic plot -----------

```{r}
###
#make scatter plot
plot(yvec~xvec, data = mydata)

#diagnostic plot
par(mfrow=c(3,2))
plot(mydata.mod)

#Leverage and Standardized Residuals table
table <- data.frame(Case = 1:nrow(mydata), 
                    xvec = mydata$xvec,
                    yvec = mydata$yvec,
                    Residuals = mydata.mod$residuals,
                    leverage = lm.influence(mydata.mod)$hat,
                    StdResiduals = rstandard(mydata.mod))
table

leverage_points <- subset(table, leverage > (4/nrow(mydata)))
leverage_points

outliers <- subset(leverage_points, abs(StdResiduals) > 2)
outliers

# check leverage points using Cooks Distance
subset(cooks.distance(mydata.mod), cooks.distance(mydata.mod) > 4/(nrow(mydata)- 2))

```

#### (b) Build `mydata` again but change it so that one data point has **high leverage** when you refit the model.  This may require trial and error.  Display the leverages for your final version of `mydata` and identify the data point with high leverage.  

##### ----------The observation numbered as 5 is the high leverage data point.-----------

```{r}
###rebuid
xvec <- c(2.2, 4.0, 5.6, 8.1, 45)

#yvec <- ???
yvec <- c(8.2, 7.1, 6.3, 3.2, 1) 

#data frame
mydata <- data.frame(xvec, yvec)

mydata.mod <- lm(yvec~xvec, data = mydata)
plot(yvec~xvec, data = mydata)


#plotting 
par(mfrow=c(3, 2))
plot(mydata.mod)

#Leverage and Standardized Residuals table
table <- data.frame(Case = 1:nrow(mydata), 
                    xvec = mydata$xvec,
                    yvec = mydata$yvec,
                    Residuals = mydata.mod$residuals,
                    leverage = lm.influence(mydata.mod)$hat,
                    StdResiduals = rstandard(mydata.mod))
table

leverage_points <- subset(table, leverage > (4/nrow(mydata)))
leverage_points


```


### Problem 
Use the file **airfares.exam.txt** for this problem.  (This file is not the same as **airfares.txt**.)   

#### Fit a simple linear regression model for $y$ = `Fare` as a function of $x$ = `Distance`.  Is there a significant linear relationship between `Fare` and `Distance`?  Justify your answer.  

##### ----------The p-value of the slope is 0.059930 which is greater than 0.05. Also, the model can only explain 21.63% of Distance on Fare, therefore we fail to reject the null hypothesis. There is no statistically significant linear relationship between Fare and Distance.-----------

```{r}
###
airfares.exam <- read.delim("~/Desktop/stat5310/mid term /mid term test/airfares.exam.txt")
head(airfares.exam)
dim(airfares.exam)

airfares.mod <- lm(Fare~Distance, data = airfares.exam) ##fit the model
summary(airfares.mod) ##display the result
```

### Problem 
#### (a) Run the code below to build data.frame `Thermometers` with columns `Celsius` and `Fahrenheit`.  Fit the simple linear regression model where `Celsius` is the predictor and `Fahrenheit` is the response.  Make a scatter plot of the data. Plot each data point as a red circle. Add curves for the fitted regression line, the boundaries of a 90% confidence interval for the regression line, and the boundaries of a 90% prediction interval for Fahrenheit.  Make the curves different colors and identify them in your discussion.  A legend is not required.  

##### ----------As we can see in the plot, the blue lines color represents for the boundaries of a 90% confidence interval and the deeppink lines color represents for a 90% prediction interval; it is WIDER than the confidence interval boundaries. .-----------

```{r}
set.seed(24681012)
Celsius <- c(-5, 0, 4, 11, 17, 22, 30, 37) 
Fahrenheit <- 32 + 1.8*Celsius + rnorm(length(Celsius), 0, 8)
Thermometers <- data.frame(Celsius, Fahrenheit)
# Fit the SLR model
thermometers.mod <- lm(Fahrenheit~Celsius, data=Thermometers)
summary(thermometers.mod)

# Make scatter plot and add curves

plot(Fahrenheit~Celsius, data=Thermometers, pch=16, col='red')
xvec <- seq(min(Thermometers$Celsius), max(Thermometers$Celsius), by =  5)
matplot(xvec, predict.lm(thermometers.mod, newdata = data.frame(Celsius = xvec), interval = 'conf', level = 0.90), col = c('black', rep('blue',2)), type = 'l', add = T)
matplot(xvec, predict.lm(thermometers.mod, newdata = data.frame(Celsius = xvec), interval = 'pred', level = 0.90), col = c('black', rep('deeppink',2)), type = 'l', add = T)


```

#### (b) What is the true value of parameter $\beta_0$ in this problem?  What about parameter $\sigma$?  

##### ----------The true value of parameter beta_0 is 32. The value for sigma is 8 -----------

### Problem 
Use the file **trees.csv** for this problem.  This dataset measures diameter (in inches), circumference (in inches), and volume (in cubic feet) for 70 shortleaf pine trees.  

#### (a) Fit a simple linear regression model called `tree_model_1` to predict circumference as a function of diameter.  Use the model to predict `Circ` when `Diam` = 21 inches.  (`tree_model_1` also gets used in the next problem)

##### ----------By using the model, we can make the prediction that when value of Diam is 21, the value for Circ is 71.60467 -----------


```{r}
### Read trees.csv file
trees <- read.csv("~/Desktop/stat5310/mid term /mid term test/trees.csv")
head(trees)

###fit SLR model

tree_model_1 <- lm(Circ ~ Diam, data = trees)
summary(tree_model_1)

###predict Circ when Diam = 21 inches
predict.lm(tree_model_1, data.frame(Diam = 21))
```



#### (b) Find the total sum of squares, the residual sum of squares, and the regression sum of squares based on `tree_model_1`. Write a sentence where you give the value of each of these quantities. 

##### ----------the total sum of squares is 17282.52, the residual sum of squares is 2770.654, and the regression sum of squares is 14511.87 -----------

```{r}
###
tree.aov = anova(tree_model_1)

SSR <- tree.aov$`Sum Sq`[1] ## Regression sum of squares
SSR

SSE <- tree.aov$`Sum Sq`[2] ## Residual sum of squares
SSE

SST <- SSR + SSE ## Total sum of squares
SST

```

### Problem
This problem also uses **trees.csv**.  

#### (a) Fit a SLR model called `tree_model_2` where the response is  `Vol` and the predictor is `Diam`.  Use the model to answer these questions:  Which points have high leverage?  Which points are outliers?  Which points are influential based on Cook's distance?  

##### ----------Observations numbered as 67, 68, 69, 70 are considered as high leverage. The outlier is the observation #70. 
              Based on Cook's distance, we indentified observations numbered as 68, 69, 70 are influential points. -----------

```{r}
### Fit a SLR model called tree_model_2
tree_model_2 <-  lm(Vol~Diam, data=trees)
plot(Vol~Diam, data=trees)

### Leverage and Standardized Residuals table
table <- data.frame(Case = 1:nrow(trees), 
                    Diam = trees$Diam,
                    Distance = trees$Vol,
                    Residuals = tree_model_2$residuals,
                    leverage = lm.influence(tree_model_2)$hat,
                    StdResiduals = rstandard(tree_model_2))
table

leverage_points <- subset(table, leverage > (4/nrow(trees))) ### detecting leverage points
leverage_points

outliers <- subset(leverage_points, abs(StdResiduals) > 2) ### identifying outliers
outliers

subset(cooks.distance(tree_model_2), cooks.distance(tree_model_2) > 4/(nrow(trees)- 2)) ### influential points using Cooks Distance


### diagnostic plots
par(mfrow=c(2,2))
plot(tree_model_2)
```

#### (b) Plot the residuals of `tree_model_2` versus $x=$ `Diam`.  What does the plot suggest about the validity of `tree_model_2`?  

##### ----------From the residuals vs fitted model, it's not a straight line, so between predictor and response does not show a linear relationship. Linerity assumption is violated, therefore this model is invalid. -----------

```{r}
### Residuals of tree_model_2 vs x 

plot(resid(tree_model_2)~Diam, data=trees, col="deeppink")
```
#### (c) Make and interpret the same plot for `tree_model_1`.  

##### ----------From the residuals vs fitted model, the trend line is close to the dashed line, so between predictor and response show a linear relationship. The linearity seems to hold reasonably well, therefore this model is valid. .-----------

```{r}
###Residuals of tree_model_1 vs x 
plot(resid(tree_model_1)~Diam, data=trees, col="deeppink")
```


### Problem
This problem uses the penguin data from earlier.  

#### Split the penguin data into two subsets, one for female penguins and one for males.  Repeat parts (a), (b), (c), and (d) from the earlier problem for each subset.  
```{r}
## split up penguin data
penguins.male <- penguins.exam %>% filter(sex == "male") ### males penguins 
head(penguins.male)
dim(penguins.male)


penguins.female <- penguins.exam %>% filter(sex == "female") ### female penguins 
head(penguins.female)
dim(penguins.female)



```

```{r}

## male penguins data set

penguins.male.mod <- lm(flipper_length_mm~bill_length_mm, data = penguins.male) ### fit the model
summary(penguins.male.mod) ##display the result

confint(penguins.male.mod, level = 0.98) ### CI's interval

```
#### (b) What proportion of the variation in flipper length is explained by bill length? 

##### ----------As we can see from the display of regression output, the model can explain the 24.69 %.-----------

#### (c) What is the estimate of $\sigma$, the standard deviation of the errors? 

##### ----------The estimate sigma is 4.912 -----------

#### (d) Make a plot with the data points and the fitted regression line.  

```{r}
plot(flipper_length_mm~bill_length_mm, data = penguins.male, col='deeppink'); abline(penguins.mod, col = 'darkblue', lwd = 2)
grid()
```

```{r}
## female penguins data set

penguins.female.mod <- lm(flipper_length_mm~bill_length_mm, data = penguins.female) ##fit the model
summary(penguins.female.mod) ##display the result

confint(penguins.female.mod, level = 0.98) ### CI's interval


```
#### (b) What proportion of the variation in flipper length is explained by bill length? 

##### ----------As we can see from the display of regression output, the model can explain the 24.69 %.-----------

#### (c) What is the estimate of $\sigma$, the standard deviation of the errors? 

##### ----------The estimate sigma is 4.912 -----------

#### (d) Make a plot with the data points and the fitted regression line.  

```{r}
plot(flipper_length_mm~bill_length_mm, data = penguins.female, col='deeppink'); abline(penguins.mod, col = 'darkblue', lwd = 2)
grid()
```



#### Rework the thermometer problem but make `Fahrenheit` the predictor and `Celsius` the response.  Values for `Fahrenheit` are given to help you get started. 

```{r}
set.seed(24681012)
Fahrenheit <- c(23, 32, 39.2, 51.8, 62.6, 71.6, 86, 98.6)
Celsius <- (Fahrenheit - 32) * 5/9 + rnorm(length(Fahrenheit), 0, 8)
Thermometers <- data.frame(Fahrenheit, Celsius)
### 


# Fit the SLR model
thermometers.mod1 <- lm(Celsius~Fahrenheit, data=Thermometers)
summary(thermometers.mod1)


# Make scatter plot and add curves

plot(Celsius~Fahrenheit, data=Thermometers, col='red')
xvec <- seq(min(Thermometers$Fahrenheit), max(Thermometers$Fahrenheit), by =  5)
matplot(xvec, predict.lm(thermometers.mod1, newdata = data.frame(Fahrenheit = xvec), interval = 'conf', level = 0.90), col = c('black', rep('blue',2)), type = 'l', add = T)
matplot(xvec, predict.lm(thermometers.mod1, newdata = data.frame(Fahrenheit = xvec),  interval = 'pred', level = 0.90), col = c('black', rep('deeppink',2)), type = 'l', add = T)

### As we can see in the plot, the blue lines color represents for the boundaries of a 90% confidence interval and the deeppink lines color represents for a 90% prediction interval; it is WIDER than the confidence interval boundaries. 
```



